{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d932b3b6",
   "metadata": {},
   "source": [
    "# Pong with PPO algorithm :D\n",
    "\n",
    "Train an agent to play pong! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe5121",
   "metadata": {},
   "source": [
    "First, let's install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]\n",
    "!pip install gym[accept-rom-license]\n",
    "!pip install torchvision\n",
    "!pip install ipython\n",
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6127d",
   "metadata": {},
   "source": [
    "Now, we can start setting up the game! Let's create the environment... (this is taken from https://www.gymlibrary.dev/environments/atari/pong/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6e7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from parallelEnv import parallelEnv \n",
    "env = gym.make('PongDeterministic-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f93d9",
   "metadata": {},
   "source": [
    "Notice this environment is Deterministic, meaning it produces the same result for a particular input. This is easier to model, but a stochastic environment would be better. **Why do you think that is?** (Hint: https://www.gymlibrary.dev/environments/atari/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa47e8d",
   "metadata": {},
   "source": [
    "The environment was created! Now let's explore... try finding out what the observation space and action space are! (Hint: https://www.gymlibrary.dev/content/basic_usage/). **What do you think this observation space corresponds to?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ffef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------TO DO---------------#\n",
    "print(\"The state (observation) space is: \", None)\n",
    "\n",
    "print(\"The action space is: \", None)\n",
    "#--------------------------------#\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61289a",
   "metadata": {},
   "source": [
    "As you could see, the action space consists of 6 possible actions the agent can perform. However, in this model we will only use two actions 'RIGHTFIRE' = 4, and 'LEFTFIRE' = 5. This way, our policies can be simple (output the probability of going to the right, so that the probability of going to the left will just be 1 minus that probability). The 'FIRE' term of the actions are just to ensure that the game starts again after you lose a life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a0af2",
   "metadata": {},
   "source": [
    "Good! Now we know a little bit about how the environment works. Let's create the policy the agent will have to follow. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$. Don't worry, this is already implemented for you, just run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Necessary Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        # (80-6)/2 +1 =38  --> 38x38x4\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        # (38-6)/4 +1 = 0 ---> 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d74ef6",
   "metadata": {},
   "source": [
    "Here we can better visualize the structure of our policy, and we also define our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30effe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "policy = Policy().to(device)\n",
    "print(policy)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6279a86",
   "metadata": {},
   "source": [
    "Now, let's define some functions that will be useful in order to visualize our environment, and play in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089287a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT=4\n",
    "LEFT=5\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display \n",
    "from matplotlib import animation\n",
    "import random as rand\n",
    "# convert outputs of parallelEnv to inputs to pytorch neural net\n",
    "# this is useful for batch processing \n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)\n",
    "\n",
    "\n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "\n",
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "\n",
    "    display(display_animation(fanim)) \n",
    "    \n",
    "# play a game and display the animation\n",
    "# nrand = number of random steps before using the policy\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    env.reset()\n",
    "\n",
    "    # start game\n",
    "    env.step(1)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done, _, _ = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done, _, _ = env.step(0)\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for _ in range(time):\n",
    "        \n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "        frame1, _, is_done, _, _ = env.step(action)\n",
    "        frame2, _, is_done, _, _ = env.step(0)\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    animate_frames(anim_frames)\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d46f67",
   "metadata": {},
   "source": [
    "Great! We can use our play() function defined before to visualize our Pong game and how our agent does with the policy we created. In this Pong game, you control the right paddle (your agent is the green paddle), and you compete against the left paddle controlled by the computer. You each try to keep deflecting the ball away from your goal and into your opponentâ€™s goal.\n",
    "\n",
    "Keep in mind that the policy hasn't been trained, so right now, the agent will mostly just do random actions when playing. You can change the 'time' parameter in the function below to visualize more or less frames, whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, policy, time=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de702cf2",
   "metadata": {},
   "source": [
    "As we can see, our agent is a very bad player right now. Let's try actually implementing the PPO algorithm to try and make our agent beat the computer! **Here we will define this PPO function, and your job is to complete the lines of code that are implementing the clip step of the algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "RIGHT=4\n",
    "LEFT=5\n",
    "\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards, discount=0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------TO DO---------------------------------------------#\n",
    "    # clipped function\n",
    "    clip = torch.clamp(None) #use the epsilon variable here\n",
    "    clipped_surrogate = torch.min(None) #you need to use the clip variable here\n",
    "    #-----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+(1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f546d",
   "metadata": {},
   "source": [
    "Nice job! Now you have your PPO algorithm all done. For this training, we will use batch processing of multiple environments to try and make our agent learn even faster. Let's first define the function that will get the trajectories of each of these environments. Just run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from parallelEnv import parallelEnv \n",
    "RIGHT=4\n",
    "LEFT=5\n",
    "\n",
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        #print( envs.step(np.random.choice([RIGHT, LEFT],n)))\n",
    "        fr1, re1, _, info = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, info = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, info = envs.step(action)\n",
    "        fr2, re2, is_done, info = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be rectangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, action_list, reward_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b228f01",
   "metadata": {},
   "source": [
    "We are now ready to train our policy!\n",
    "\n",
    "Depending on your CPU, this may take from 45 minutes to 2 hours. \n",
    "\n",
    "**While this trains, briefly explain in your own words what is happening here during this training phase. Look closely at each line of code inside the two 'for' cycles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0a420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "RIGHT=4\n",
    "LEFT=5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 400\n",
    "\n",
    "# widget bar to display progress\n",
    "\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "\n",
    "\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=42)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "  \n",
    "    old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    for _ in range(SGD_epoch):\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    \n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8b4da",
   "metadata": {},
   "source": [
    "Nice! Now your policy is all trained up. Let's visualize how the rewards your agent obtained changed each episode. **Copy and paste this graph in your document and explain what is happening.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeaf224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bbd50",
   "metadata": {},
   "source": [
    "It looks like your agent obtained decent rewards in the later episodes. That's what we wanted! If you trained for a higher number of episodes, your agent might have gotten even better rewards. But I'm sure this training you did will be enough to beat the computer now... let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, policy, time=600) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c29f",
   "metadata": {},
   "source": [
    "Yay! Our agent did amazing against the computer. You can now save the policy you trained for future occasions. And if you ever want to use it again, you can just load it back up! Thanks for playing Pong :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4802353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab504f",
   "metadata": {},
   "source": [
    "# BONUS\n",
    "\n",
    "Try out training your policy with 2 different number of episodes as before (they can be higher or lower than 400). Put the mean rewards graph you obtained in your document and analyze why they look like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315c4de",
   "metadata": {},
   "source": [
    "***IMPORTANT: Before uploading the modified .ipybn files be sure to clear all the outputs.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
