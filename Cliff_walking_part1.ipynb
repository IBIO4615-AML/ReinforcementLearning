{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530f2def",
   "metadata": {},
   "source": [
    "# Cliff Walking with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d45e91",
   "metadata": {},
   "source": [
    "In this part of the homework we will explore the Q-learning algorithm with the Cliff Walking environment from the Gym library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4dca5",
   "metadata": {},
   "source": [
    "First, install all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym[toy_text]\n",
    "%pip install gym\n",
    "%pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed48bb",
   "metadata": {},
   "source": [
    "Now import the libraries that we are going to use in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57530ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed239545",
   "metadata": {},
   "source": [
    "The first step in our implementation corresponds to creating the environment. The render mode \"human\" is the argument that will allow us to visualize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf58f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"CliffWalking-v0\", render_mode=\"human\").env "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6339014",
   "metadata": {},
   "source": [
    "The environment was created! Now let's explore... \n",
    "- **Find out the observation space**\n",
    "- **Find out the action space**\n",
    "- **Explain what do these concepts mean in this environment**\n",
    "\n",
    "Hint: https://www.gymlibrary.dev/content/basic_usage/\n",
    "- **Finally, explore the environment documentation and tell us how the reward is defined.**\n",
    "\n",
    "Hint: https://www.gymlibrary.dev/environments/toy_text/cliff_walking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------TO DO---------------#\n",
    "state_space = None\n",
    "#--------------------------------#\n",
    "\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "#------------TO DO---------------#\n",
    "action_space = None\n",
    "#--------------------------------#\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb094a5",
   "metadata": {},
   "source": [
    "Now, we'll implement an algorithm for visualizing the environmnet before the training step. IMPORTANT: The 'pretraining' argument is the one that will allow us to implement the code in this stage of the homework. As we haven't defined the Q table yet, the q_table_cw parameter will be 0.\n",
    "\n",
    "- **Please describe what is happening in the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(episodes, max_steps, q_table_cw, pretraining=False):\n",
    "\n",
    "    list_rewards= []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, info =env.reset()\n",
    "        done=False\n",
    "        print(\"EPISODE \", episode+1,)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        current_reward=0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            if pretraining==True:\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                action = np.argmax(q_table_cw[state,:])\n",
    "                \n",
    "            new_state, reward, terminated, truncated ,info = env.step(action)\n",
    "            \n",
    "            current_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                print(\"Terminated or truncated\")\n",
    "                clear_output(wait=True)\n",
    "                break\n",
    "\n",
    "            state=new_state\n",
    "            list_rewards.append(current_reward)\n",
    "        print(\"Episode's reward:\", current_reward)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return list_rewards\n",
    "\n",
    "\n",
    "# Visualize the environment before the training \n",
    "\n",
    "trying = visualize(5,15,0, pretraining=True)\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa29337",
   "metadata": {},
   "source": [
    "Now, **complete the function in order to initialize our Q-table as zeroes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b13e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    #------------TO DO---------------#\n",
    "    Qtable = None\n",
    "    #--------------------------------#\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13637df",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_cw = initialize_q_table(state_space, action_space)\n",
    "print(\"Q-table shape: \", q_table_cw.shape)\n",
    "print(\"Q-table: \", q_table_cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd91f6",
   "metadata": {},
   "source": [
    "Now, we will create the environment again without the Render argument. This is done in order to implement the training in a much faster way. We will also define the parameters that will be needed in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b01b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"CliffWalking-v0\").env \n",
    "\n",
    "\n",
    "# Training parameters\n",
    "training_episodes = 50000     # Total training episodes\n",
    "learning_rate = 0.1           # Learning rate\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 20              # Max steps per episode\n",
    "gamma = 0.95                # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0           # Exploration probability at start\n",
    "min_epsilon = 0.05          # Minimum exploration probability\n",
    "decay_rate = 0.001          # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa19ec",
   "metadata": {},
   "source": [
    "Now we will implement our epsilon_greedy_policy algorithms. **Please explain the relation of the next two functions with the \"Exploration\" and \"Exploitation\" Concepts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ee6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q_table_cw, state):\n",
    "    action = np.argmax(q_table_cw[state][:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  \n",
    "    random_num = random.uniform(0,1)\n",
    "  \n",
    "    if random_num > epsilon:\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375aa54f",
   "metadata": {},
   "source": [
    "Now we are ready to implement our training process!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, q_table_cw):\n",
    "    \n",
    "    for episode in tqdm(range(training_episodes)):\n",
    "        \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\n",
    "        \n",
    "    # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "\n",
    "          # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(q_table_cw, state, epsilon)\n",
    "\n",
    "          # Take action At and observe Rt+1 and St+1\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "          # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            q_table_cw[state,action] = q_table_cw[state,action] + learning_rate * (reward + gamma * np.max(q_table_cw[new_state,:]) - q_table_cw[state,action])\n",
    "\n",
    "          # If terminated or truncated finish the episode\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "\n",
    "              # Our next state is the new state\n",
    "            state = new_state\n",
    "    return q_table_cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac120b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_cw = train(training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, q_table_cw)\n",
    "q_table_cw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c41e8",
   "metadata": {},
   "source": [
    "Finally, we create the environment once again in order to visualize it. Note that now we are using the updated Q-table and therefore we won't need the \"pretraining\" parameter.\n",
    "\n",
    "You should be able to visualize the results now!!!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e26ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"CliffWalking-v0\", render_mode=\"human\").env #Creation of the environment for visualization\n",
    "\n",
    "\n",
    "rewards = visualize(5,20,q_table_cw)\n",
    "\n",
    "\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca232d",
   "metadata": {},
   "source": [
    "We hope you enjoyed this part of the homework. \n",
    "\n",
    "Now is your turn to experiment:\n",
    "- **Run at least 8 experiments varying the number of training episodes and any other parameters you find interesting. Please add a table with the final rewards obtained in your experiments and an analysis of the results in your document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb90bf2",
   "metadata": {},
   "source": [
    "***IMPORTANT: Before uploading the modified .ipybn files be sure to clear all the outputs.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
